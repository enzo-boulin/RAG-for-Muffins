import json
import logging
import os
import random
import re
import time

import httpx
from bs4 import BeautifulSoup

from muffin.recipe import Ingredient

URLS_FILE = "data/muffin_links.txt"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36"
}
FAILED_LOG = "data/failed_urls.txt"

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


def get_marmiton_json(url: str) -> dict | None:
    """
    R√©cup√®re les donn√©es JSON-LD d'une recette Marmiton √† partir de son URL.
    Retourne un dictionnaire avec les donn√©es de la recette ou None en cas d'√©chec
    """
    # D√©finition des headers pour simuler un navigateur
    headers = HEADERS

    # 1. R√©cup√©ration de la page
    response = httpx.get(url, headers=headers)
    response.raise_for_status()

    # 2. Analyse du HTML avec BeautifulSoup
    soup = BeautifulSoup(response.text, "html.parser")

    # 3. Extraction du script JSON-LD
    elements = soup.find_all("script", type="application/ld+json")

    for element in elements:
        if element.string is None:
            continue

        data = json.loads(element.string)

        # V√©rification si le @type est "Recipe" on a trouv√© le bon JSON
        if isinstance(data, dict) and data.get("@type") == "Recipe":
            return data


def get_recipe_urls(
    query: str = "muffin", nb_pages: int = 1, save_to_file: str | None = None
) -> list[str]:
    """
    R√©cup√®re les URLs des recettes Marmiton en fonction d'une requ√™te de recherche.
    Args:
        query (str): Terme de recherche.
        nb_pages (int): Nombre de pages de r√©sultats √† parcourir.
        save_to_file (str | None): Chemin du fichier pour sauvegarder les URLs. Si None, ne sauvegarde pas.
    Returns:
        list[str]: Liste des URLs des recettes trouv√©es.
    """
    base_url = "https://www.marmiton.org/recettes/recherche.aspx"
    recipe_links = set()  # Utilisation d'un set pour √©viter les doublons

    headers = HEADERS

    for page in range(1, nb_pages + 1):
        logger.info(f"‚è≥ Collecte de la page {page}...")
        params = {"aqt": query, "page": page}

        try:
            response = httpx.get(base_url, params=params, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # Sur Marmiton, les liens de recettes sont dans des balises <a>
            # On cherche les liens qui contiennent "/recettes/recette_"
            for a in soup.find_all("a", href=True):
                href = str(a["href"])
                if "/recettes/recette_" in href:
                    recipe_links.add(
                        href
                        if href.startswith("http")
                        else "https://www.marmiton.org" + href
                    )

            # Pour √™tre plus discret
            time.sleep(1)

        except Exception as e:
            logger.warning(f"Erreur sur la page {page}: {e}")
            break

    if save_to_file:
        with open(save_to_file, "w") as f:
            for link in recipe_links:
                f.write(link + "\n")

    return list(recipe_links)


def run_scraper():
    """Lance le scrapper pour r√©cup√©rer les recettes Marmiton list√©es dans le fichier URLS_FILE."""
    # 1. Lire les URLs
    with open(URLS_FILE, "r") as f:
        urls = [line.strip() for line in f if line.strip()]

    logger.info(f"üò± Total URLs √† traiter : {len(urls)}")

    for url in urls:
        # 2. Extraire un ID unique de l'URL pour le nom de fichier
        # Exemple: .../recette_muffins-au-chocolat_165038.aspx -> 165038
        try:
            recipe_id = url.split("_")[-1].split(".")[0]
        except IndexError:
            logger.warning(f"‚ùå Format d'URL inattendu : {url}")
            recipe_id = str(hash(url))  # Fallback si format URL bizarre

        file_path = "data/raw_recipes/" + f"recipe_{recipe_id}.json"

        logger.info(f"‚è≥ T√©l√©chargement : {url}")
        data = get_marmiton_json(url)

        if data:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)

            # Pour la discr√©tion
            time.sleep(random.uniform(1, 3))
        else:
            with open(FAILED_LOG, "a") as f:
                f.write(f"{url}\n")

    logger.info("‚úÖ Processus termin√© !")


def get_all_existing_ingredients(
    filepath: str = "data/raw_recipes", save_to: str = "data/"
) -> set[str]:
    """Parcourt tous les fichiers JSON dans le r√©pertoire sp√©cifi√©,
    extrait les ingr√©dients et les sauvegarde dans un fichier texte.
    Args:
        filepath (str): Chemin du r√©pertoire contenant les fichiers JSON.
        save_to (str): Chemin du r√©pertoire o√π sauvegarder le fichier texte.
    Returns:
        set[str]: Ensemble des ingr√©dients extraits.
    """
    all_ingredients: set[str] = set()

    for file in os.listdir(filepath):
        if not file.endswith(".json"):
            continue
        with open(os.path.join(filepath, file), "r", encoding="utf-8") as f:
            data = json.load(f)
            ingredients = data.get("recipeIngredient", [])
            all_ingredients |= set(ingredients)

    with open(save_to + "raw_ingredients.txt", "w") as f:
        for ingredient in all_ingredients:
            f.write(ingredient + "\n")

    return all_ingredients


def fraction_to_float(value_str: str) -> float:
    """
    Convertit une cha√Æne (fraction ou d√©cimal) en float.
    G√®re "3/4", "0.75" et "0,75".
    """
    normalized_str = value_str.replace(",", ".")

    if "/" in normalized_str:
        parts = normalized_str.split("/")
        if len(parts) == 2:
            return float(parts[0]) / float(parts[1])

    return float(normalized_str)


def clean_ingredient(
    raw_ingredient: str,
) -> Ingredient:
    """Nettoie une cha√Æne d'ingr√©dient brut et retourne un objet Ingredient."""
    units = [
        # --- VOLUMES & CONTENANTS ---
        r"(?:verres?|tasses?|bols?|pots?|bocaux|briques?|briquettes?|bo√Ætes?)",
        r"(?:barquettes?|paquets?|sachets?|tablettes?|portions?)",
        r"(?:cl|ml|dl|l|kg|g)\b",  # Unit√©s m√©triques avec bordure de mot
        # --- CUILL√àRES (Variantes complexes) ---
        # Capture : cuill√®res √† soupe, bonnes cuill√®res √† caf√©, demi cuill√®res √† caf√©, etc.
        r"(?:[a-z√¢√©√®]+ )?cuill√®res?(?: √† (?:soupe|caf√©|th√©))?",
        r"√† th√©",  # Cas isol√©s
        # --- D√âCOUPE & FORMES ---
        r"(?:tranches?(?: √©paisses)?|lamelles?|rondelles?|d√©s|morceaux?|carr√©s?)",
        r"(?:gousses?|feuilles?|branches?|brins?|bouquets?|p√©pites?|traits?)",
        r"(?:pointes?|portions?)",
        # --- MESURES MANUELLES & PR√âCISION ---
        r"(?:(?:grosses |petites )?pinc√©es?)",
        r"(?:(?:grosses |petites )?poign√©es?)",
        r"(?:gouttes?)",
        # --- UNIT√âS G√âN√âRIQUES & FRACTIONS ---
        r"unit√©\(s\)",
        r"demis?",  # Pour "1 demi de levure"
        r"sachets?",
    ]

    units_pattern = r"|".join(units)

    # Regex principale pour extraire QTY, UNIT et NAME
    regex = rf"^(?P<qty>\d+[\s\./]\d+|\d+(?:[\.,]\d+)?)?\s*(?P<unit>\b(?:{units_pattern})\b)?\s*(?:de\s+|d['‚Äô]\s*)?(?P<name>.*)"

    match = re.match(regex, raw_ingredient.strip(), re.IGNORECASE)

    if not match:
        raise ValueError(f"Impossible de parser l'ingr√©dient : {raw_ingredient}")

    raw_qty = match.group("qty")
    qty = fraction_to_float(raw_qty) if raw_qty else None

    unit = match.group("unit") or None
    name = match.group("name").strip()

    # 1. Coupe √† la premi√®re parenth√®se ouvrante
    name = re.sub(r"\s*\(.*", "", name)

    # 2. Coupe aux points de suspension (...)
    name = re.sub(r"\s*\.\.\..*", "", name)

    # --- NOUVELLE R√àGLE : Conjonctions et symboles ---
    # On cherche " et/ou ", " ou ", " et " ... (avec \b pour les mots entiers) ou le signe "+"
    # Puis on coupe tout ce qui suit (.*)
    name = re.sub(
        r"\s*(?:\bet/ou\b|\bou\b|\bet\b|\bplus\b|\bavec\b|\bpour\b|\bdans\b|\+).*",
        "",
        name,
        flags=re.IGNORECASE,
    )

    # 3. Nettoyage final des pr√©positions et espaces
    name = re.sub(r"^[dD]['‚Äô]\s*", "", name).strip()

    return Ingredient(
        name=name,
        quantity=qty,
        unit=unit,
    )
