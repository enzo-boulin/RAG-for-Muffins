import logging

import chromadb
import ollama

from muffin.constant import CHROMADB_PATH, COLLECTION_NAME, LOGGING_LEVEL
from muffin.models import (
    RecipeModel,
    SentenceTransformerEmbeddingFunction,
    SessionLocal,
    convert_model_to_dataclass,
)

logger = logging.getLogger(__name__)
logging.basicConfig(level=LOGGING_LEVEL)


def final_prompt(user_prompt: str, str_recipe: str) -> str:
    """
    G√©n√®re une r√©ponse structur√©e et d√©taill√©e en utilisant le mod√®le Mistral.

    Args:
        user_prompt: La question ou les ingr√©dients de l'utilisateur.
        str_recipe: La cha√Æne de caract√®res contenant les donn√©es de la recette (contexte).

    Returns:
        str: La r√©ponse format√©e du Chef Muffin.
    """
    # Le message syst√®me d√©finit le comportement de l'IA avec des contraintes de structure
    system_prompt: str = """
    TU ES "CHEF MUFFIN", UN ASSISTANT CULINAIRE OBSESSIONNEL MAIS SYMPATHIQUE.
    TON OBJECTIF EST DE FOURNIR LA RECETTE D√âTAILL√âE DU MUFFIN TROUV√â DANS LE CONTEXTE.

    ### TES DIRECTIVES (GUARDRAILS) :
    1. OBSESSION : Tu ne cuisines QUE des muffins. Refuse tout autre plat avec humour.
    2. LANGUE : R√©ponds en fran√ßais app√©tissant.
    3. EXHAUSTIVIT√â : Tu DOIS lister TOUS les ingr√©dients et leurs QUANTIT√âS exactes mentionn√©s dans le contexte.
    4. STRUCTURE DE R√âPONSE OBLIGATOIRE :
       - Un titre accrocheur.
       - Une section "üõí INGR√âDIENTS" avec une liste √† puces (quantit√©s incluses).
       - Une section "üë®‚Äçüç≥ PR√âPARATION" avec les √©tapes num√©rot√©es d√©taillant chaque action.
       - Une astuce de chef ou un mot de fin chaleureux.

    ### INTERDICTION :
    - Ne r√©sume pas la recette. 
    - N'invente pas d'√©tapes si elles ne sont pas dans le contexte.
    """

    augmented_prompt: str = f"""
    CONTEXTE (Donn√©es brutes de la recette) :
    {str_recipe}

    QUESTION DE L'UTILISATEUR :
    {user_prompt}
    
    INSTRUCTION : Produis la recette compl√®te en respectant la structure impos√©e.
    """

    response = ollama.chat(
        model="mistral",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": augmented_prompt},
        ],
    )

    return str(response["message"]["content"])


# # --- Exemple d'utilisation ---
# ma_recette_db = (
#     "Muffins au cacao : 200g farine, 50g cacao, 2 oeufs. Cuire 20min √† 180¬∞C."
# )
# ma_question = "J'ai du cacao et de la farine, qu'est-ce que je peux faire ?"

# reponse_finale = synthetiser_reponse(ma_question, ma_recette_db)
# print(reponse_finale)


def main():
    client = chromadb.PersistentClient(path=CHROMADB_PATH)

    collection = client.get_collection(
        name=COLLECTION_NAME,
        embedding_function=SentenceTransformerEmbeddingFunction(),
    )

    user_prompt = input(
        "üòéDis moi ce qu'il y a dans ton frigo, je te dirai quel muffin cuisiner.ü§™ \nüëâ"
    )
    results = collection.query(query_texts=[user_prompt], n_results=1)
    with SessionLocal() as session:
        logger.info("‚è≥ Chargement de la recette depuis SQLite...")
        recipe_model = (
            session.query(RecipeModel).filter_by(id=int(results["ids"][0][0])).one()
        )
        recipe = convert_model_to_dataclass(recipe_model)
    chief_answer = final_prompt(user_prompt, str(recipe))
    print(chief_answer)


if __name__ == "__main__":
    main()
